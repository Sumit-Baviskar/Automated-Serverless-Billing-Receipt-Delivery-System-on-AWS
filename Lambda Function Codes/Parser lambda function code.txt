import boto3
import csv
import json
import os
import urllib.parse

# Create AWS clients
s3 = boto3.client("s3")
sqs = boto3.client("sqs")

# Get SQS queue URL from environment variable
SQS_URL = os.environ["SQS_URL"]

def lambda_handler(event, context):
    # Log the received event for debugging
    print("âœ… Event received:", json.dumps(event))

    # Get the first record (in case multiple records are uploaded at once)
    record = event["Records"][0]
    bucket = record["s3"]["bucket"]["name"]
    key = urllib.parse.unquote_plus(record["s3"]["object"]["key"])

    print(f"ðŸª£ Bucket: {bucket}, ðŸ“„ Key: {key}")

    # Get the object (CSV file) from S3
    response = s3.get_object(Bucket=bucket, Key=key)
    csv_content = response["Body"].read().decode("utf-8").splitlines()
    reader = csv.DictReader(csv_content)

    # Dictionary to store customer-wise data
    customer_data = {}

    # Iterate over each row in the CSV
    for row in reader:
        cust_id = row["customer_id"]
        cust_name = row["customer_name"]
        email = row["email"]
        service_name = row["service_name"]
        charges = float(row["charges"])

        # Initialize data for new customer if not already present
        if cust_id not in customer_data:
            customer_data[cust_id] = {
                "customer_id": cust_id,
                "customer_name": cust_name,
                "email": email,
                "services": []
            }

        # Append service details for the customer
        customer_data[cust_id]["services"].append({
            "service_name": service_name,
            "charges": charges
        })

    # Send each customer's data as a separate message to SQS
    for cust_id, data in customer_data.items():
        sqs.send_message(
            QueueUrl=SQS_URL,
            MessageBody=json.dumps(data)
        )
        print(f"ðŸ“¤ Message pushed to SQS for customer: {cust_id}")

    return {
        "statusCode": 200,
        "body": "âœ… Messages pushed to SQS for each customer"
    }